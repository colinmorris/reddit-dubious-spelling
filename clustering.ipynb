{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167294, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kaepernick</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adderall</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reddiquette</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acid</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>park</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         token   n\n",
       "0   kaepernick  87\n",
       "1     adderall  73\n",
       "2  reddiquette  73\n",
       "3         acid  72\n",
       "4         park  68"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('token_counts.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: got a token without type str: nan\n",
      "Ran in 13.5s\n",
      "adderall : {'adderall': 73, 'aderal': 10, 'adderral': 9, 'aderall': 16, 'adderol': 17, 'adderoll': 6, 'adderal': 46, 'adderrall': 8} total = 185\n",
      "alzheimers : {'alzheimers': 36, \"alzheimer's\": 8, 'altzheimers': 6} total = 50\n",
      "asbergers : {\"asperger's\": 7, 'asbergers': 36, 'aspbergers': 15, 'asburgers': 9, 'aspergers': 31} total = 98\n",
      "comradery : {'comeraderie': 8, 'camaraderie': 6, 'comaraderie': 5, 'comraderie': 24, 'comradery': 54, 'cameraderie': 7, 'comradere': 8} total = 112\n",
      "deschanel : {'dechanel': 8, 'deschutes': 6, 'deshanel': 7, 'deschanel': 48} total = 69\n",
      "dilaudid : {'dilauded': 23, 'diladid': 5, 'dilaudid': 41, 'delaudid': 5} total = 74\n",
      "faire : {'fare': 9, 'fair': 13, 'faire': 36} total = 58\n",
      "fentanyl : {'fentynol': 21, 'fentanol': 9, 'fentenyl': 5, 'phentanyl': 5, 'fentanyl': 42, 'fentynal': 9, 'fentynl': 5} total = 96\n",
      "galifinakis : {'galafanakis': 8, 'galifianakis': 14, 'galifinakis': 52, 'galifanakis': 7, 'galafinakis': 17} total = 98\n",
      "gras : {'gra': 5, 'gray': 27, 'gras': 34, 'gravis': 5} total = 71\n",
      "gyllenhal : {'gylenhal': 9, 'gyllenhall': 41, 'gylenhaal': 8, 'gylenhall': 18, 'gyllenhaal': 29, 'gyllenhal': 41, 'gillenhall': 5} total = 151\n",
      "heimlich : {'heimlich': 54} total = 54\n",
      "johansen : {'johansen': 44, 'johannson': 14, 'johanssen': 20, 'johannsen': 16, 'johansson': 31, 'johanson': 33} total = 158\n",
      "kaepernick : {'kapernick': 43, 'kaepernick': 87, 'kaepernik': 6, 'kapernik': 10} total = 146\n",
      "kardashian : {'kardasian': 9, 'kardashin': 7, 'kardashian': 42} total = 58\n",
      "loogie : {'loogy': 9, 'loogie': 45, 'lugie': 6, 'loogey': 6} total = 66\n",
      "mache : {'mache': 36} total = 36\n",
      "mcconaughey : {'mcconaughey': 37, 'mcconnahey': 6, 'mcconnahay': 5, 'mcconnaughey': 5, 'mccaunaghey': 6, 'mcconahey': 15, 'mccounaghey': 7, 'mcconaghey': 23, 'mcconaughy': 9, 'mcconahay': 8} total = 121\n",
      "minaj : {'minage': 8, 'minaj': 50, 'manaj': 11} total = 69\n",
      "palahniuk : {'pahlaniuk': 12, 'palahniuk': 44, 'palaniuk': 5, 'palahnuik': 9} total = 70\n",
      "pescatarian : {'pescatarian': 34, 'pescetarian': 17} total = 51\n",
      "reddiquette : {'reddiquitte': 17, 'rediquitte': 5, 'rediquette': 10, 'reddiquite': 16, 'redditquite': 5, 'redditquette': 8, 'reddiquette': 73, 'reddiquete': 16} total = 150\n",
      "sarkeesian : {'sarkeesian': 61, 'sarkisian': 10, 'sarkesian': 22, 'sarkessian': 8, 'sarkeesan': 8} total = 109\n",
      "seizure : {'seizures': 11, 'siezure': 30, 'seizure': 43} total = 84\n",
      "shamalan : {'shaymalan': 6, 'shamalan': 36, 'shymalan': 7, 'shamalayan': 11, 'shyamalan': 27, 'shamalyan': 10, 'shamaylan': 5, 'shamylan': 14} total = 116\n",
      "silmarillion : {'simarillion': 9, 'silmarillion': 35, 'silmarilion': 6} total = 50\n",
      "soleil : {'soleil': 34, 'soliel': 12, 'solei': 27} total = 73\n",
      "sriracha : {'saracha': 5, 'sriracha': 42, 'sriacha': 11, 'siracha': 32, 'sirracha': 11} total = 101\n",
      "tyson : {'tyson': 51} total = 51\n",
      "welbutrin : {'welbutrin': 37, 'wellbutrin': 20} total = 57\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import distance\n",
    "import time\n",
    "\"\"\"\n",
    "Algo strawman...\n",
    "\n",
    "canon = {}\n",
    "For each token...\n",
    "    find canon form with lowest edit distance to token\n",
    "    if edit distance < some thresh:\n",
    "        add this to canon's children\n",
    "    elif n >= some other thresh:\n",
    "        add this as a new canon term\n",
    "\n",
    "What should edit distance thresh be?\n",
    "Well lev(\"camaraderie\", \"comradery\") = 4. 5 for \"comrodery\".\n",
    "\n",
    ">>> j1 = 'johansen'\n",
    ">>> j2 = 'johansson'\n",
    ">>> j3 = 'johanson'\n",
    ">>> lev = distance.levenshtein\n",
    ">>> lev(j1, j2)\n",
    "2\n",
    ">>> lev(j1, j2, normalized=True)                                                                                                                                                                      \n",
    "0.2222222222222222\n",
    ">>> lev(j1, j3, normalized=True)                                                                                                                                                                      \n",
    "0.125\n",
    ">>> lev(j2, j3, normalized=True)                                                                                                                                                                      \n",
    "0.1111111111111111\n",
    "\n",
    "camar/comrad norm dist = .3636...\n",
    "\n",
    "\n",
    "High level note: this doesn't need to be *perfect*. Can always do some manual correction and\n",
    "fine-tuning after (which might be cheaper than over-optimizing the heuristics).\n",
    "Also, relatively easy to fine-tune an alphabetized list as long as it's not too long, since groups that\n",
    "should be merged should be close to each other (will almost always start with same 1 or 2 chars)\n",
    "\"\"\"\n",
    "\n",
    "# Remove some false positive terms from consideration\n",
    "blist = {'park', 'acid', 'vong', 'bay', 'syndrome', 'disease', 'effect', 'bay', 'island', 'khan', 'lee'}\n",
    "\n",
    "# Post-hoc filtering of some spurious matches\n",
    "blist2 = ('bechamel claire fairley faile fire fairy farrel carfentanyl hansen'\n",
    "          ' johnson kardashians karazhan bougie loogies roofie loose coozie'\n",
    "          ' rachel machine machete lachey michel matcha'\n",
    "          ' mccauley mcdonough mcdonaugh mcdonagh pescatarians etiquette ettiquette'\n",
    "          ' shavasana shanahan sole tyrion'\n",
    "         ).split()\n",
    "blist2 = set(blist2)\n",
    "blist = blist.union(blist2)\n",
    "def naive_cluster(df, max_distance=5, max_clusters=1000, norm=False, min_n=2):\n",
    "    \"\"\"Return a dictionary where the values are sets of similar word forms. The key is the word\n",
    "    form in that set with the greatest number of appearances.\n",
    "    \n",
    "    norm: whether to normalize edit distance\n",
    "    min_n: only look at word forms with at least this many occurrences\n",
    "    max_clusters: once the dict to be returned has at least this many entries, stop adding \n",
    "        new keys (but continue iterating through word forms and comparing them to the existing keys)\n",
    "    \"\"\"\n",
    "    canon = {}\n",
    "    pairs = zip(df.token, df.n)\n",
    "    for token, n in pairs:\n",
    "        if n < min_n:\n",
    "            break\n",
    "        if not isinstance(token, str):\n",
    "            print(\"Warning: got a token without type str: {!r}\".format(token))\n",
    "            continue\n",
    "        if len(token) <= 2:\n",
    "            continue\n",
    "        if token in blist:\n",
    "            continue\n",
    "        # Avoid false-positive associations like Kardashian -> Kardashian's\n",
    "        if token.endswith(\"'s\") and not (token.startswith('alz') or token.startswith('asb') \n",
    "                                         or token.startswith('asp')):\n",
    "            continue\n",
    "        best = (99, None)\n",
    "        # Find closest canonical form (could use ilevenshtein - not sure if it's faster)\n",
    "        for cand in canon.keys():\n",
    "            lev = distance.levenshtein(token, cand, \n",
    "                                       max_dist=max_distance,\n",
    "                                       normalized=norm\n",
    "                                      )\n",
    "            if (not norm and lev != -1) or lev < max_distance:\n",
    "                if best[0] < 99:\n",
    "                    # Could try to be really fancy here and do a merge.\n",
    "                    print(\"Ambiguous match for token {}. {} has score {}. {} has score {}\".format(\n",
    "                        token, cand, lev, best[1], best[0]\n",
    "                    ))\n",
    "                if lev < best[0]:\n",
    "                    best = (lev, cand)\n",
    "        if best[0] < 99:\n",
    "            canon[best[1]][token] = n\n",
    "        # If we didn't find a match and there's still room, add this token as a new canonical form\n",
    "        elif len(canon) < max_clusters:\n",
    "            canon[token] = {token: n}\n",
    "    return canon\n",
    "\n",
    "t0 = time.time()\n",
    "c = naive_cluster(df, max_distance=.37, norm=True, max_clusters=30, min_n=5)\n",
    "print(\"Ran in {:.1f}s\".format(time.time()-t0))\n",
    "for canon, forms in sorted(c.items(), key=lambda tup: tup[0]):\n",
    "    if len(forms) > 1 or True:\n",
    "        total = sum(forms.values())\n",
    "        print(canon, ':', forms, 'total =', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adderall 185\n",
      "johansen 158\n",
      "gyllenhal 151\n",
      "reddiquette 150\n",
      "kaepernick 146\n",
      "mcconaughey 121\n",
      "shamalan 116\n",
      "comradery 112\n",
      "sarkeesian 109\n",
      "sriracha 101\n",
      "galifinakis 98\n",
      "asbergers 98\n",
      "fentanyl 96\n",
      "seizure 84\n",
      "dilaudid 74\n",
      "soleil 73\n",
      "gras 71\n",
      "palahniuk 70\n",
      "minaj 69\n",
      "deschanel 69\n",
      "loogie 66\n",
      "kardashian 58\n",
      "faire 58\n",
      "welbutrin 57\n",
      "heimlich 54\n",
      "tyson 51\n",
      "pescatarian 51\n",
      "alzheimers 50\n",
      "silmarillion 50\n",
      "mache 36\n"
     ]
    }
   ],
   "source": [
    "for k, v in sorted(c.items(), key=lambda tup: sum(tup[1].values()), reverse=True):\n",
    "    total = sum(v.values())\n",
    "    print(k, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c8a0f2862f43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('clusters.pickle', 'wb') as f:\n",
    "    pickle.dump(c, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "# https://stats.stackexchange.com/questions/123060/clustering-a-long-list-of-strings-words-into-similarity-groups\n",
    "\n",
    "# Well, this is shockingly shitty! Even after trying to tune params like preference and damping.\n",
    "def affprop_cluster(df, prefs=False):\n",
    "    words = df.token.values\n",
    "    lev_similarity = -1*np.array([[\n",
    "        distance.nlevenshtein(w1,w2) \n",
    "        for w1 in words] for w2 in words\n",
    "    ])\n",
    "    preference = -.15\n",
    "    if prefs:\n",
    "        preference = (-1/df.n) * 5\n",
    "    affprop = sklearn.cluster.AffinityPropagation(affinity=\"precomputed\", damping=0.9, \n",
    "                                                  preference=preference,\n",
    "                                                  verbose=True,\n",
    "                                                 )\n",
    "    affprop.fit(lev_similarity)\n",
    "    \n",
    "    for cluster_id in np.unique(affprop.labels_)[:50]:\n",
    "        exemplar = words[affprop.cluster_centers_indices_[cluster_id]]\n",
    "        cluster = np.unique(words[np.nonzero(affprop.labels_==cluster_id)])\n",
    "        cluster_str = \", \".join(cluster)\n",
    "        print(\" - *%s:* %s\" % (exemplar, cluster_str))\n",
    "        \n",
    "#affprop_cluster(df.head(100), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "words = df.head(5).token.values\n",
    "sim = -1*np.array([[distance.nlevenshtein(w1,w2) for w1 in words] for w2 in words])\n",
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# StackOverflow example code\n",
    "\n",
    "words = \"\"\"I have the following problem at hand: I have a very long list of words, possibly names, surnames, etc. \n",
    "I need to cluster this word list, such that similar words, for example words with similar edit (Levenshtein) \n",
    "distance appears in the same cluster. For example \"algorithm\" and \"alogrithm\" should have high chances to \n",
    "appear in the same cluster.\n",
    "\n",
    "I am well aware of the classical unsupervised clustering methods like k-means clustering, EM clustering in the Pattern Recognition literature. The problem here is that these methods work on points which reside in a vector space. I have words of strings at my hand here. \n",
    "It seems that, the question of how to represent strings in a numerical vector space and to calculate \"means\" of \n",
    "string clusters is not sufficiently answered, according to my survey efforts until now. A naive approach to attack \n",
    "this problem would be to combine k-Means clustering with Levenshtein distance, but the question still remains \n",
    "\"How to represent \"means\" of strings?\". There is a weight called as TF-IDF weight, but it seems that it is \n",
    "mostly related to the area of \"text document\" clustering, not for the clustering of single words. It seems that \n",
    "there are some special string clustering algorithms existing, like the one at \n",
    "\n",
    "My search in this area is going on still, but I wanted to get ideas from here as well. What would you do recommend in this \n",
    "case, is anyone aware of any methods for this kind of problem?\n",
    "\"\"\".split() #Replace this line\n",
    "words = np.asarray(words) #So that indexing with a list will work\n",
    "lev_similarity = -1*np.array([[distance.levenshtein(w1,w2) for w1 in words] for w2 in words])\n",
    "\n",
    "affprop = sklearn.cluster.AffinityPropagation(affinity=\"precomputed\", damping=0.5)\n",
    "affprop.fit(lev_similarity)\n",
    "for cluster_id in np.unique(affprop.labels_):\n",
    "    exemplar = words[affprop.cluster_centers_indices_[cluster_id]]\n",
    "    cluster = np.unique(words[np.nonzero(affprop.labels_==cluster_id)])\n",
    "    cluster_str = \", \".join(cluster)\n",
    "    print(\" - *%s:* %s\" % (exemplar, cluster_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
